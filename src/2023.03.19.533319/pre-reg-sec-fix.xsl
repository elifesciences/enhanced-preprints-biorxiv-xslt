<?xml version="1.0" encoding="UTF-8"?>
<xsl:stylesheet xmlns:xsl="http://www.w3.org/1999/XSL/Transform"
    xmlns:xs="http://www.w3.org/2001/XMLSchema"
    xmlns:xlink="http://www.w3.org/1999/xlink"
    exclude-result-prefixes="xs"
    version="3.0">
    
    <xsl:output method="xml" encoding="UTF-8"/>
    
    <xsl:template match="*|@*|text()">
        <xsl:copy>
            <xsl:apply-templates select="*|@*|text()"/>
        </xsl:copy>
    </xsl:template>
    
    <xsl:template match="article[//article-meta/article-version='1.2']/back/sec[@id=('s7')]">
        <sec id="s7">
            <title>Investigating sex differences in learning in a range-expanding bird</title>
            <p>Alexis J. Breen<sup>1,</sup>&#x002A; &#x0026; Dominik Deffner<sup>1,2,3</sup></p>
            <p><sup>1</sup>Department of Human Behavior, Ecology and Culture, Max Planck Institute for Evolutionary Anthropology, Leipzig 04103, Germany</p>
            <p><sup>2</sup>Science of Intelligence Excellence Cluster, Technical University Berlin, Berlin 10623, Germany</p>
            <p><sup>3</sup>Center for Adaptive Rationality, Max Planck Institute for Human Development, Berlin 14195, Germany</p>
            <p>&#x002A;alexis_breen@eva.mpg.de</p>
            <sec id="s8">
                <title>Abstract</title>
                <p>How might differences in dispersal and learning interact in range expansion dynamics? To begin to answer this question, in this preregistration we detail the background, hypothesis plus associated predictions, and methods of our proposed study, including the development and validation of a mechanistic reinforcement learning model, which we aim to use to assay colour-reward reinforcement learning (and the influence of two candidate latent parameters&#x2014;speed and sampling rate&#x2014;on this learning) in great-tailed grackles&#x2014;a species undergoing rapid range expansion, where males disperse.</p>
            </sec>
            <sec id="s9">
                <title>Introduction</title>
                <p>Dispersal and range expansion go &#x2018;hand in hand&#x2019;; movement by individuals away from a population&#x2019;s core is a pivotal precondition of witnessed growth in species&#x2019; geographic limits (<bold><italic><xref ref-type="bibr" rid="c20">Chuang &#x0026; Peterson, 2016</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c116">Ronce, 2007</xref></italic></bold>). Because &#x2018;who&#x2019; disperses&#x2014;in terms of sex&#x2014;varies both within and across taxa (for example, male-biased dispersal is dominant among fish and mammals, whereas female-biased dispersal is dominant among birds; see <bold><italic><xref rid="tbl1" ref-type="table">Table 1</xref></italic></bold> in <bold><italic><xref ref-type="bibr" rid="c121">Trochet et al., 2016</xref></italic></bold>), skewed sex ratios are apt to arise at expanding range fronts, and, in turn, differentially drive invasion dynamics. Female-biased dispersal, for instance, can &#x2018;speed up&#x2019; staged invertebrate invasions by increasing offspring production (<bold><italic><xref ref-type="bibr" rid="c113">Miller &#x0026; Inouye, 2013</xref></italic></bold>). Alongside sex-biased dispersal, learning ability is also argued to contribute to species&#x2019; colonisation capacity, as novel environments inevitably present novel (foraging, predation, shelter, and social) challenges that newcomers need to surmount in order to settle successfully (<bold><italic><xref ref-type="bibr" rid="c118">Sol et al., 2013</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c130">Wright et al., 2010</xref></italic></bold>). Indeed, a growing number of studies show support for this supposition (as recently reviewed in <bold><italic><xref ref-type="bibr" rid="c106">Lee &#x0026; Thornton, 2021</xref></italic></bold>). Carefully controlled choice tests, for example, show that urban-dwelling individuals&#x2014;that is, the &#x2018;invaders&#x2019;&#x2014;will both learn and unlearn novel reward-stimulus pairings more rapidly than their rural-dwelling counterparts (<bold><italic><xref ref-type="bibr" rid="c7">Batabyal &#x0026; Thaker, 2019</xref></italic></bold>), suggesting that range expansion selects for enhanced learning ability at the dispersal and/or settlement stage(s). Given the independent influence of sex-biased dispersal and learning ability on range expansion, it is perhaps surprising, then, that their potential interactive influence on this aspect of movement ecology remains unexamined, particularly as interactive links between dispersal and other behavioural traits such as aggression are documented within the range expansion literature (<bold><italic><xref ref-type="bibr" rid="c99">Duckworth, 2006</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c102">Gutowsky &#x0026; Fox, 2011</xref></italic></bold>).</p>
                <p>That learning ability can covary with, for example, exploration (e.g., <bold><italic><xref ref-type="bibr" rid="c91">Auersperg et al., 2011</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c101">Guillette et al., 2011</xref></italic></bold>) and neophobia (e.g., <bold><italic><xref ref-type="bibr" rid="c123">Verbeek et al., 1994</xref></italic></bold>), two behaviours which may likewise play a role in range expansion (<bold><italic><xref ref-type="bibr" rid="c100">Griffin et al., 2017</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c106">Lee &#x0026; Thornton, 2021</xref></italic></bold>), is one potential reason for the knowledge gap introduced above. Such correlations stand to mask what contribution, if any, learning ability lends to range expansion&#x2014;an undoubtedly daunting research prospect. A second (and not mutually exclusive) reason is that, for many species, a detailed diary of their range expansion is lacking (<bold><italic><xref ref-type="bibr" rid="c9">Blackburn et al., 2009</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c122">Udvardy &#x0026; Papp, 1969</xref></italic></bold>). And patchy population records inevitably introduce interpretive &#x2018;noise,&#x2019; imaginably impeding population comparisons of learning ability (or the like).</p>
                <p>In range-expanding great-tailed grackles (<italic>Quiscalus mexicanus</italic>), however, learning ability appears to represent a unique source of individual variation; more specifically, temporarily-captive great-tailed grackles&#x2019; speed to solve colour-reward reinforcement learning tests does not correlate with measures of their exploration (time spent moving within a novel environment), inhibition (time to reverse a colour-reward preference), motor diversity (number of distinct bill and/or feet movements used in behavioural tests), neophobia (latency to approach a novel object), risk aversion (time spent stationary within a &#x2018;safe spot&#x2019; in a novel environment), persistence (number of attempts to engage in behavioural tests), or problem solving (number of test-relevant functional and non-functional object-choices) (<bold><italic><xref ref-type="bibr" rid="c109">Logan, 2016a</xref></italic></bold>, <bold><italic><xref ref-type="bibr" rid="c110">2016b</xref></italic></bold>). Moreover, careful combing by researchers of public records, such as regional bird reports and museum collections, means that great-tailed grackle range-expansion data is both comprehensive and readily available (<bold><italic><xref ref-type="bibr" rid="c27">Dinsmore &#x0026; Dinsmore, 1993</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c114">Pandolfino et al., 2009</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c126">Wehtje, 2003</xref></italic></bold>). Thus, great-tailed grackles offer behavioural ecologists a useful study system to investigate the interplay between life-history strategies, learning ability, and range expansion.</p>
                <p>Here, for the first time (to our knowledge), we propose to investigate potential differences in colour-reward reinforcement learning performance between male and female great-tailed grackles (<bold><italic><xref rid="fig1a" ref-type="fig">Figure 1</xref></italic></bold>), to test the hypothesis that sex differences in learning ability are related to sex differences in dispersal. Since the late nineteenth century, great-tailed grackles have been expanding their range at an unprecedented rate, moving northward from their native range in Central America into the United States (breeding in at least 20 states), with several first-sightings spanning as far north as Canada (<bold><italic><xref ref-type="bibr" rid="c27">Dinsmore &#x0026; Dinsmore, 1993</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c126">Wehtje, 2003</xref></italic></bold>). Notably, the record of this range expansion in great-tailed grackles is heavily peppered with first-sightings involving a single or multiple male(s) (<bold><italic><xref ref-type="bibr" rid="c27">Dinsmore &#x0026; Dinsmore, 1993</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c105">Kingery, 1972</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c108">Littlefield, 1983</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c120">Stepney, 1975</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c126">Wehtje, 2003</xref></italic></bold>). Moreover, recent genetic data show that, when comparing great-tailed grackles within a population, average relatedness: (<italic>i</italic>) is higher among females than among males; and (<italic>ii</italic>) decreases with increasing geographic distance among females; but (<italic>iii</italic>) is unrelated to geographic distance among males; hence, confirming a role for male-biased dispersal in great-tailed grackles (Sevchik et al., in press). Considering these natural history and genetic data, then, we expect male and female great-tailed grackles to differ across at least two colour-reward reinforcement learning parameters: speed and sampling rate (here, sampling is defined as switching between choice-options). Specifically, we expect male&#x2014;versus female&#x2014;great-tailed grackles: (prediction 1 &#x0026; 2) to be faster to, firstly, learn a novel colour-reward pairing, and secondly, reverse their colour preference when the colour-reward pairing is swapped; and (prediction 3) to be more deterministic&#x2014;that is, sample less often&#x2014;in their colour-reward learning; if learning ability and dispersal relate. Indeed, since invading great-tailed grackles face agribusiness-led wildlife management strategies, including the use of chemical crop repellents (<bold><italic><xref ref-type="bibr" rid="c128">Werner et al., 2011</xref></italic></bold>, <bold><italic><xref ref-type="bibr" rid="c127">2015</xref></italic></bold>), range expansion should disfavour slow, error-prone learning strategies, resulting in a spatial sorting of learning ability in great-tailed grackles (<bold><italic><xref ref-type="bibr" rid="c130">Wright et al., 2010</xref></italic></bold>). Related to this final point, we further expect (prediction 4) such sex differences in learning ability to be more pronounced in great-tailed grackles living at the edge, rather than the intermediate and/or core, region of their range (e.g., <bold><italic><xref ref-type="bibr" rid="c99">Duckworth, 2006</xref></italic></bold>).</p>
                <fig id="fig1a" position="float" orientation="portrait" fig-type="figure">
                    <label>Figure 1</label>
                    <caption><p>Left panel: images showing a male and female great-tailed grackle (credit: Wikimedia Commons). Right panel: schematic of the colour-reward reinforcement learning experimental protocol. In the <italic>initial learning</italic> phase, great-tailed grackles are presented with two colour-distinct tubes; however, only one coloured tube (e.g., dark grey) contains a food reward (F&#x002B; versus F-). In the <italic>reversal learning</italic> phase, the colour-reward tube-pairings are swapped. The passing criterion was identical in both phases (see main text for details).</p></caption>
                    <graphic xlink:href="533319v2_fig1a.tif"/>
                </fig>
            </sec>
            <sec id="s10">
                <title>Methods</title>
                <sec id="s10a">
                    <title>Data</title>
                    <p>This preregistration aims to use colour-reward reinforcement learning data collected (or being collected) in great-tailed grackles across three study sites that differ in their range-expansion demographics; that is, belonging to a core, intermediate, or edge population (based on time-since-settlement population growth dynamics, as outlined in <bold><italic><xref ref-type="bibr" rid="c20">Chuang &#x0026; Peterson, 2016</xref></italic></bold>). Specifically, data will be utilised from: (<italic>i</italic>) Tempe, Arizona&#x2014;hereafter, the core population (estimated&#x2014;by adding the average time between first sighting and first breeding to the year first sighted&#x2014;to be breeding since 1951) (<bold><italic><xref ref-type="bibr" rid="c124">Walter, 2004</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c126">Wehtje, 2003</xref></italic></bold>); (<italic>ii</italic>) Santa Barbara, California&#x2014;hereafter, the intermediate population (known to be breeding since 1996) (<bold><italic><xref ref-type="bibr" rid="c107">Lehman, 2020</xref></italic></bold>); and (<italic>iii</italic>) Woodland, California&#x2014;hereafter, the edge population (known to be breeding since 2004) (<bold><italic><xref ref-type="bibr" rid="c103">Hampton, 2001</xref></italic></bold>). Data collection at both the Tempe, Arizona and Santa Barbara, California study sites has been completed prior to the submission of this preregistration (total sample size across sites: nine females and 25 males); however, data collection at the Woodland, California study site is ongoing (current sample size: three females and nine males; anticipated minimum total sample size: five females and ten males). Thus, the final data set should contain colour-reward reinforcement learning data from at least 14 female and 35 male great-tailed grackles.</p>
                </sec>
                <sec id="s10b">
                    <title>Experimental protocol</title>
                    <sec id="s10b1">
                        <title>General</title>
                        <p>A step-by-step description of the experimental protocol is reported elsewhere (e.g., <bold><italic><xref ref-type="bibr" rid="c10">Blaisdell et al., 2021</xref></italic></bold>). As such, below we detail only the protocol for the colour-reward reinforcement learning tests that we propose to analyse herein.</p>
                    </sec>
                    <sec id="s10b2">
                        <title>Colour-reward reinforcement learning tests</title>
                        <p>The reinforcement learning tests consist of two phases (<bold><italic><xref rid="fig1a" ref-type="fig">Figure 1</xref></italic></bold>, right panel): (<italic>i</italic>) colour-reward learning (hereafter, initial learning) and (<italic>ii</italic>) colour-reward reversal learning (hereafter, reversal learning). In both phases, two different coloured tubes are used: for Santa Barbara great-tailed grackles, gold and grey (<bold><italic><xref ref-type="bibr" rid="c110">Logan, 2016b</xref></italic></bold>, <bold><italic><xref ref-type="bibr" rid="c109">2016a</xref></italic></bold>); for all other great-tailed grackles: light and dark grey (<bold><italic><xref ref-type="bibr" rid="c10">Blaisdell et al., 2021</xref></italic></bold>). Each tube consists of an outer and inner diameter of 26 mm and 19 mm, respectively; and each is mounted to two pieces of plywood attached at a right angle (entire apparatus: 50 mm wide &#x00D7; 50 mm tall &#x00D7; 67 mm deep); thus resulting in only one end of each coloured tube being accessible (<bold><italic><xref rid="fig1a" ref-type="fig">Figure 1</xref></italic></bold>, right panel).</p>
                        <p>In the <italic>initial learning phase</italic>, great-tailed grackles are required to learn that only one of the two coloured tubes contains a food reward (e.g., dark grey; this colour-reward pairing is counterbalanced across great-tailed grackles within each study site). Specifically, the rewarded and unrewarded coloured tubes are placed&#x2014;either on a table or on the floor&#x2014;in the centre of the aviary run (distance apart: table, 2 ft; floor, 3 ft), with the open tube-ends facing, and perpendicular to, their respective aviary side-wall. Which coloured tube is placed on which side of the aviary run (left or right) is pseudorandomised across trials. A trial begins at tube-placement, and ends when a great-tailed grackle has either made a tube-choice or the maximum trial time has elapsed (eight minutes). A tube-choice is defined as a great-tailed grackle bending down to examine the contents (or lack thereof) of a tube. If the chosen tube contains food, the great-tailed grackle is allowed to retrieve and eat the food, before both tubes are removed and the rewarded coloured tube is rebaited out of sight (for the great-tailed grackle). If a chosen tube does not contain food, both tubes are immediately removed. Each great-tailed grackle is given, first, up to three minutes to make a tube-choice (after which a piece of food is placed equidistant between the tubes to entice participation); and then, if no choice has been made, an additional five minutes maximum, before both tubes are removed. All trials are recorded as either correct (choosing the rewarded colour tube), incorrect (choosing the unrewarded colour tube), or incomplete (no choice made); and are presented in 10-trial blocks. To pass initial learning, a great-tailed grackle must make a correct choice in at least 17 out of the most recent 20 trials, with a minimum of eight and nine correct choices across the last two blocks.</p>
                        <p>In the <italic>reversal learning phase</italic>, great-tailed grackles are required to learn that the colour-reward pairing has been switched; that is, the previously unrewarded coloured tube (e.g., light grey) now contains a food reward. The protocol for this second and final learning phase is identical to that, described above, of the initial learning phase.</p>
                    </sec>
                </sec>
                <sec id="s10c">
                    <title>Analysis plan</title>
                    <sec id="s10c1">
                        <title>General</title>
                        <p>Here, we will analyse, process, and visually present our data using, respectively, the &#x2018;rstan&#x2019; (<bold><italic><xref ref-type="bibr" rid="c119">Stan Development Team, 2020</xref></italic></bold>), &#x2018;rethinking&#x2019; (<bold><italic><xref ref-type="bibr" rid="c111">McElreath, 2018</xref></italic></bold>), and &#x2018;tidyverse&#x2019; (<bold><italic><xref ref-type="bibr" rid="c129">Wickham et al., 2019</xref></italic></bold>) packages in R (<bold><italic><xref ref-type="bibr" rid="c115">R Core Team, 2021</xref></italic></bold>). Our reproducible code is available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/alexisbreen/Sex-differences-in-grackles-learning">https://github.com/alexisbreen/Sex-differences-in-grackles-learning</ext-link>).</p>
                    </sec>
                    <sec id="s10c2">
                        <title>Reinforcement learning model</title>
                        <p>In this preregistration, we propose to employ an adapted (from <bold><italic><xref ref-type="bibr" rid="c25">Deffner et al., 2020</xref></italic></bold>) Bayesian reinforcement learning model, to examine the influence of sex on great-tailed grackles&#x2019; initial and reversal learning performance. The reinforcement learning model, defined below, allows us to link observed coloured tube-choices to latent individual-level knowledge-updating (of attractions towards, learning about, and sampling of, either coloured tube) based on recent tube-choice reward-payoffs, and to translate such latent knowledge-updating into individual tube-choice probabilities; in other words, we can reverse engineer the probability that our parameters of interest (speed and sampling rate) produce great-tailed grackles&#x2019; observed tube-choice behaviour by formulating our scientific model as a statistical model (<bold><italic><xref ref-type="bibr" rid="c111">McElreath, 2018</xref></italic></bold>, p. 537). This method can therefore capture whether, and, if so, how multiple latent learning strategies simultaneously guide great-tailed grackles&#x2019; decision making&#x2014;an analytical advantage over more traditional methods (e.g., comparing trials to passing criterion) that ignore the potential for equifinality (<bold><italic><xref ref-type="bibr" rid="c92">Barrett, 2019</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c104">Kandler &#x0026; Powell, 2018</xref></italic></bold>).</p>
                        <p>Our reinforcement learning model consists of two equations:</p>
                        <disp-formula id="eqn1a">
                            <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="533319v2_eqn1a.gif"/>
                        </disp-formula>
                        <disp-formula id="eqn1b">
                            <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="533319v2_eqn1b.gif"/>
                        </disp-formula>
                        <p><bold><italic><xref rid="eqn1a" ref-type="disp-formula">Equation 1</xref></italic></bold> expresses how attraction (<italic>A</italic>) to a choice-option (<italic>i</italic>) changes for an individual (<italic>j</italic>) across time (<italic>t</italic> &#x002B; 1) based on their prior attraction to that choice-option (<italic>A</italic><sub><italic>i</italic>,<italic>j</italic>,<italic>t</italic></sub>) plus their recently experienced choice-payoff (&#x03C0;<sub><italic>i</italic>,<italic>j</italic>,<italic>t</italic></sub>), whilst accounting for the weight given to recent payoffs (&#x03C6;<sub><italic>k</italic>,<italic>l</italic></sub>). As &#x03C6;<sub><italic>k</italic>,<italic>l</italic></sub> increases in value, so, too, does the rate of individual attraction-updating; thus, &#x03C6;<sub><italic>k</italic>,<italic>l</italic></sub> represents the individual learning rate. We highlight that the <italic>k</italic>, <italic>l</italic> indexing denotes that we estimate separate &#x03C6; parameters for each phase of the experiment (<italic>k</italic> &#x003D; 1 for initial, <italic>k</italic> &#x003D; 2 for reversal) and each sex (<italic>l</italic> &#x003D; 1 for females, <italic>l</italic> &#x003D; 2 for males).</p>
                        <p><bold><italic><xref rid="eqn1b" ref-type="disp-formula">Equation 2</xref></italic></bold> is a softmax function that expresses the probability (<italic>P</italic>) that option (<italic>i</italic>) is selected in the next choice-round (<italic>t</italic> &#x002B; 1) as a function of the attractions and a parameter (&#x03BB;<sub><italic>k</italic>,<italic>l</italic></sub>) that governs how much relative differences in attraction scores guide individual choice-behaviour. The higher the value of &#x03BB;<sub><italic>k</italic>,<italic>l</italic></sub>, the more deterministic (less option-switching) the choice-behaviour of an individual becomes (note &#x03BB;<sub><italic>k</italic>,<italic>l</italic></sub> &#x003D; 0 generates random choice); thus, &#x03BB;<sub><italic>k</italic>,<italic>l</italic></sub> represents the individual sampling rate for phase <italic>k</italic> and sex <italic>l</italic>.</p>
                        <p>From the above reinforcement learning model, then, we will generate inferences about the effect of sex on &#x03C6;<sub><italic>k</italic>,<italic>l</italic></sub> and &#x03BB;<sub><italic>k</italic>,<italic>l</italic></sub> from at least 1000 effective samples of the posterior distribution (see our model validation below). We note that our reinforcement learning model also includes both individual bird and study site as random effects (to account for repeated measures within both individuals and populations); however, for clarity, these parameters are omitted from our equations (but not our code: <ext-link ext-link-type="uri" xlink:href="https://github.com/alexisbreen/Sex-differences-in-grackles-learning">https://github.com/alexisbreen/Sex-differences-in-grackles-learning</ext-link>). Regarding our study site random effect, we further note that, as introduced above, we will also explore population-mediated sex-effects on &#x03C6; and &#x03BB;, by comparing these learning parameters both within and between sexes at each study site. Finally, our reinforcement learning model excludes trials where a great-tailed grackle did not make a tube-choice, as this measure cannot clearly speak to individual learning ability&#x2014;for example, satiation rather than any learning of &#x2018;appropriate&#x2019; colour tube-choice could be invoked as an explanation in such cases. Indeed, there are, admittedly, a number of intrinsic and extrinsic factors (e.g., temperament and temperature, respectively) that might bias great-tailed grackles&#x2019; tube-choice behaviour, and, in turn, the output from our reinforcement learning model (<bold><italic><xref ref-type="bibr" rid="c125">Webster &#x0026; Rutz, 2020</xref></italic></bold>). Nonetheless, our reinforcement learning model serves as a useful first step towards addressing if learning ability and dispersal relate in great-tailed grackles (for a similiar rationale, see <bold><italic><xref ref-type="bibr" rid="c112">McElreath &#x0026; Smaldino, 2015</xref></italic></bold>).</p>
                    </sec>
                    <sec id="s10c3">
                        <title>Model validation</title>
                        <p>We validated our reinforcement learning model in three steps. First, we performed agent-based simulations. Specifically, we followed the tube-choice behaviour of simulated great-tailed grackles&#x2014;that is, 14 females and 35 males from one of three populations (where population membership matched known study site sex distributions)&#x2014;across the described initial learning and reversal learning phases. The tube-choice behaviour of the simulated great-tailed grackles was governed by a set of rules identical to those defined by our mathematical equations&#x2014;for example, coloured tube attractions were independently updated based on the reward outcome of tube choices. Because we assigned higher average &#x03C6; and &#x03BB; values to simulated male (versus female) great-tailed grackles, the resulting data set should show males outperform females on initial and reversal learning, at both the group and individual-level; it did (<bold><italic><xref rid="fig2a" ref-type="fig">Figure 2</xref></italic></bold> &#x0026; S1, respectively).</p>
                        <fig id="fig2a" position="float" orientation="portrait" fig-type="figure">
                            <label>Figure 2</label>
                            <caption><p>Group-level tube-choice behaviour of simulated great-tailed grackles across colour-reward reinforcement learning trials (females: yellow, <italic>n</italic> &#x003D; 14; males: green, <italic>n</italic> &#x003D; 35), following model validation step one. Tube option 1 (e.g., dark grey) was the rewarded option in the initial learning phase; conversely, tube option 2 (e.g., light grey) contained the food reward in the reversal learning phase. Each open circle represents an individual tube-choice; black lines indicate binomial smoothed conditional means fitted with grey 89&#x0025; compatability intervals.</p></caption>
                            <graphic xlink:href="533319v2_fig2a.tif"/>
                        </fig>
                        <p>Next, we ran our simulated data set on our reinforcement learning model. Here, we endeavored to determine whether our reinforcement learning model: (<italic>i</italic>) recovered our assigned &#x03C6;<sub><italic>k</italic>,<italic>l</italic></sub> and &#x03BB;<sub><italic>k</italic>,<italic>l</italic></sub> values (it did; <bold><italic><xref rid="tbl1" ref-type="table">Table 1</xref></italic></bold>); and (<italic>ii</italic>) produced &#x2018;correct&#x2019; qualitative inferences&#x2014;that is, detected the simulated sex differences in great-tailed grackles&#x2019; initial and reversal learning (it did; <bold><italic><xref rid="fig3a" ref-type="fig">Figure 3</xref></italic></bold>).</p>
                        <table-wrap id="tbl1" orientation="portrait" position="float">
                            <label>Table 1:</label>
                            <caption><p>Comparison of assigned and recovered &#x03C6; and &#x03BB; values, following model validation step two. Eighty-nine percent highest posterior density intervals (HPDI) are shown for recovered values.</p></caption>
                            <graphic xlink:href="533319v2_tbl1.tif"/>
                        </table-wrap>
                        <fig id="fig3a" position="float" orientation="portrait" fig-type="figure">
                            <label>Figure 3</label>
                            <caption><p>Comparison of learning ability in simulated female (yellow; <italic>n</italic> &#x003D; 14) and male (green; <italic>n</italic> &#x003D; 35) great-tailed grackles across initial and reversal colour-reward reinforcement learning, following model validation step two. (<bold>A</bold>) &#x03C6;, the rate of learning i.e., speed. (<bold>B</bold>) &#x03BB;, the rate of sampling i.e., switching between choice-options. (<bold>C</bold>) and (<bold>D</bold>) show posterior distributions for respective contrasts between female and male learning. Eighty-nine percent highest posterior density intervals are shaded in grey; that this interval does not cross zero evidences a simulated effect of sex on learning ability.</p></caption>
                            <graphic xlink:href="533319v2_fig3a.tif"/>
                        </fig>
                        <p>Finally, we repeated step one and step two, using a range of realistically plausible &#x03C6; and &#x03BB; sex differences (note that values for female great-tailed grackles were left unchanged from <bold><italic><xref rid="tbl1" ref-type="table">Table 1</xref></italic></bold>), to determine whether our reinforcement learning model could detect different effect sizes of sex on our target learning parameters. This final step confirmed that, for our anticipated minimum sample size, our reinforcement learning model: (<italic>i</italic>) detects sex differences in &#x03C6; values &#x003E;&#x003D; 0.03 and &#x03BB; values &#x003E;&#x003D; 1; and (<italic>ii</italic>) infers a null effect for &#x03C6; values &#x003C; 0.03 and &#x03BB; values &#x003C; 1 i.e., very weak simulated sex differences (<bold><italic><xref rid="fig4a" ref-type="fig">Figure 4</xref></italic></bold>). Both of these points together highlight how our reinforcement learning model allows us to say that null results are not just due to small sample size. Additionally, estimates obtained from step three were more precise in the reversal learning phase compared to the initial learning phase (<bold><italic><xref rid="fig4a" ref-type="fig">Figure 4</xref></italic></bold>), and we can expect to detect even smaller sex differences if we analyse learning across both phases&#x2014;an approach we will apply if we detect no effect of phase. In sum, model validation steps one through three confirm that our reinforcement learning model is reasonably fit.</p>
                        <fig id="fig4a" position="float" orientation="portrait" fig-type="figure">
                            <label>Figure 4</label>
                            <caption><p>Parameter recovery test for different sizes of simulated sex differences. Plots show posterior estimates of the effect of sex (contrasts between simulated male and female great-tailed grackles; <italic>n</italic> &#x003D; 14 and 35, respectively) on speed (&#x03C6;) and sampling (&#x03BB;) learning parameters, following model validation step three. Black circles represent the mean recovered sex effect estimates with grey eighty-nine percent highest posterior density intervals (HPDIs); black solid diagonal lines represent a &#x2018;perfect&#x2019; match between assigned and recovered parameter estimates (note that we would not expect a perfect correspondence due to stochasticity of agent-based simulations); and black dashed horizontal lines represent a recovered null sex effect.</p></caption>
                            <graphic xlink:href="533319v2_fig4a.tif"/>
                        </fig>
                    </sec>
                </sec>
                <sec id="s10d">
                    <title>Bias</title>
                    <p>AJB and DD are (at the time of submitting this preregistration) blind with respect to all but two aspects of the target data: the sex and population membership of each grackle that has, thus far, completed, or is expected to complete, the colour-reward reinforcement learning tests (because these parameters were used in model validation simulations&#x2014;see above).</p>
                </sec>
                <sec id="s10e">
                    <title>Open materials</title>
                    <p><ext-link ext-link-type="uri" xlink:href="https://github.com/alexisbreen/Sex-differences-in-grackles-learning">https://github.com/alexisbreen/Sex-differences-in-grackles-learning</ext-link></p>
                </sec>
            </sec>
            <sec>
                <title>Acknowledgements</title>
                <p>We thank all members, past and present, of the Grackle Project for collecting and sharing the data that we propose to analyse herein. We further thank Richard McElreath for study support.</p>
            </sec>
            <sec id="s11">
                <title>Ethics</title>
                <p>All data utilised herein were collected with ethical approval.</p>
            </sec>
            <sec id="s12">
                <title>Supplementary material</title>
                <fig id="figs1" position="float" orientation="portrait" fig-type="figure">
                    <label>Figure S1</label>
                    <caption><p>Individual-level tube-choice behaviour of simulated great-tailed grackles across colour-reward reinforcement learning trials (females: yellow, <italic>n</italic> &#x003D; 14; males: green, <italic>n</italic> &#x003D; 35). Tube option 1 (e.g., dark grey) was the rewarded option in the initial learning phase; conversely, tube option 2 (e.g., light grey) contained the food reward in the reversal learning phase. Each open circle shows an individual tube-choice; black solid lines show loess smoothed conditional means fitted with grey 89&#x0025; compatibility intervals; and dashed black lines show individual-unique transitions between learning phases.</p></caption>
                    <graphic xlink:href="533319v2_figs1.tif"/>
                </fig>
                <fig id="figs2" position="float" orientation="portrait" fig-type="figure">
                    <label>Figure 2&#x2014;figure supplement 1.</label>
                    <caption><p>Comparison of information-updating rate &#x03C6; and risk-sensitivity rate &#x03BB; estimates (top and bottom row, respectively) in initial learning excluding and including extra initial learning trials (left and right column, respectively), which are present in the original data set (see Methods and materials). Because this comparison does not show any noticeable difference depending on their inclusion or exclusion, we excluded extra learning trials from our analyses. All plots are generated via model estimates using our full sample size: 32 males and 17 females.</p></caption>
                    <graphic xlink:href="533319v2_figs2.tif"/>
                </fig>
            </sec>
        </sec>
    </xsl:template>
    
    <xsl:template match="article[//article-meta/article-version='1.2']/back/sec[@id=('s8','s9','s10','s11','s12')]"/>
    
</xsl:stylesheet>